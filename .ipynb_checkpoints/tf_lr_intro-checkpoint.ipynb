{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LDrzLFXE8T1l"
   },
   "source": [
    "# Introduction to Logistic Regression Using TF 2.0\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "1. Build a model,\n",
    "2. Train this model on example data, and\n",
    "3. Use the model to make predictions about unknown data.\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this lab, you use machine learning to *categorize* Iris flowers by species.  It uses TensorFlow to:\n",
    "\n",
    "* Use TensorFlow's default [eager execution](../../guide/eager.ipynb) development environment,\n",
    "* Import data with the [Datasets API](../../guide/datasets.ipynb),\n",
    "* Build models and layers with TensorFlow's [Keras API](../../guide/keras/overview.ipynb).\n",
    "\n",
    "This tutorial is structured like many TensorFlow programs:\n",
    "\n",
    "1. Import and parse the dataset.\n",
    "2. Select the type of model.\n",
    "3. Train the model.\n",
    "4. Evaluate the model's effectiveness.\n",
    "5. Use the trained model to make predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yNr7H-AIoLOR"
   },
   "source": [
    "## Setup program"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1J3AuPBT9gyR"
   },
   "source": [
    "### Configure imports\n",
    "\n",
    "Import TensorFlow and the other required Python modules. By default, TensorFlow uses [eager execution](../../guide/eager.ipynb) to evaluate operations immediately, returning concrete values instead of creating a computational graph that is executed later. If you are used to a REPL or the `python` interactive console, this feels familiar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jElLULrDhQZR"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bfV2Dai0Ow2o"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g4Wzg69bnwK2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.4.1\n",
      "Eager execution: True\n"
     ]
    }
   ],
   "source": [
    "print(\"TensorFlow version: {}\".format(tf.__version__))\n",
    "print(\"Eager execution: {}\".format(tf.executing_eagerly()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zx7wc0LuuxaJ"
   },
   "source": [
    "## The Iris classification problem\n",
    "\n",
    "Imagine you are a botanist seeking an automated way to categorize each Iris flower you find. Machine learning provides many algorithms to classify flowers statistically. For instance, a sophisticated machine learning program could classify flowers based on photographs. Our ambitions are more modestâ€”we're going to classify Iris flowers based on the length and width measurements of their [sepals](https://en.wikipedia.org/wiki/Sepal) and [petals](https://en.wikipedia.org/wiki/Petal).\n",
    "\n",
    "The Iris genus entails about 300 species, but our program will only classify the following three:\n",
    "\n",
    "* Iris setosa\n",
    "* Iris virginica\n",
    "* Iris versicolor\n",
    "\n",
    "<table>\n",
    "  <tr><td>\n",
    "    <img src=\"https://www.tensorflow.org/images/iris_three_species.jpg\"\n",
    "         alt=\"Petal geometry compared for three iris species: Iris setosa, Iris virginica, and Iris versicolor\">\n",
    "  </td></tr>\n",
    "  <tr><td align=\"center\">\n",
    "    <b>Figure 1.</b> <a href=\"https://commons.wikimedia.org/w/index.php?curid=170298\">Iris setosa</a> (by <a href=\"https://commons.wikimedia.org/wiki/User:Radomil\">Radomil</a>, CC BY-SA 3.0), <a href=\"https://commons.wikimedia.org/w/index.php?curid=248095\">Iris versicolor</a>, (by <a href=\"https://commons.wikimedia.org/wiki/User:Dlanglois\">Dlanglois</a>, CC BY-SA 3.0), and <a href=\"https://www.flickr.com/photos/33397993@N05/3352169862\">Iris virginica</a> (by <a href=\"https://www.flickr.com/photos/33397993@N05\">Frank Mayfield</a>, CC BY-SA 2.0).<br/>&nbsp;\n",
    "  </td></tr>\n",
    "</table>\n",
    "\n",
    "Fortunately, someone has already created a [dataset of 120 Iris flowers](https://en.wikipedia.org/wiki/Iris_flower_data_set) with the sepal and petal measurements. This is a classic dataset that is popular for beginner machine learning classification problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3Px6KAg0Jowz"
   },
   "source": [
    "## Import and parse the training dataset\n",
    "\n",
    "Download the dataset file and convert it into a structure that can be used by this Python program.\n",
    "\n",
    "### Download the dataset\n",
    "\n",
    "Download the training dataset file using the `tf.keras.utils.get_file` function. This returns the file path of the downloaded file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J6c7uEU9rjRM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local copy of the dataset file: /Users/rjdaskevich/.keras/datasets/iris_training.csv\n"
     ]
    }
   ],
   "source": [
    "train_dataset_url = \"https://storage.googleapis.com/download.tensorflow.org/data/iris_training.csv\"\n",
    "\n",
    "train_dataset_fp = tf.keras.utils.get_file(fname=os.path.basename(train_dataset_url),\n",
    "                                           origin=train_dataset_url)\n",
    "\n",
    "print(\"Local copy of the dataset file: {}\".format(train_dataset_fp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qnX1-aLors4S"
   },
   "source": [
    "### Inspect the data\n",
    "\n",
    "This dataset, `iris_training.csv`, is a plain text file that stores tabular data formatted as comma-separated values (CSV). Use the `head -n5` command to take a peek at the first five entries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FQvb_JYdrpPm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120,4,setosa,versicolor,virginica\n",
      "6.4,2.8,5.6,2.2,2\n",
      "5.0,2.3,3.3,1.0,1\n",
      "4.9,2.5,4.5,1.7,2\n",
      "4.9,3.1,1.5,0.1,0\n"
     ]
    }
   ],
   "source": [
    "!head -n5 {train_dataset_fp}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kQhzD6P-uBoq"
   },
   "source": [
    "From this view of the dataset, notice the following:\n",
    "\n",
    "1. The first line is a header containing information about the dataset:\n",
    "  * There are 120 total examples. Each example has four features and one of three possible label names.\n",
    "2. Subsequent rows are data records, one *[example](https://developers.google.com/machine-learning/glossary/#example)* per line, where:\n",
    "  * The first four fields are *[features](https://developers.google.com/machine-learning/glossary/#feature)*: these are the characteristics of an example. Here, the fields hold float numbers representing flower measurements.\n",
    "  * The last column is the *[label](https://developers.google.com/machine-learning/glossary/#label)*: this is the value we want to predict. For this dataset, it's an integer value of 0, 1, or 2 that corresponds to a flower name.\n",
    "\n",
    "Let's write that out in code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9Edhevw7exl6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\n",
      "Label: species\n"
     ]
    }
   ],
   "source": [
    "# column order in CSV file\n",
    "column_names = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species']\n",
    "\n",
    "feature_names = column_names[:-1]\n",
    "label_name = column_names[-1]\n",
    "\n",
    "print(\"Features: {}\".format(feature_names))\n",
    "print(\"Label: {}\".format(label_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CCtwLoJhhDNc"
   },
   "source": [
    "Each label is associated with string name (for example, \"setosa\"), but machine learning typically relies on numeric values. The label numbers are mapped to a named representation, such as:\n",
    "\n",
    "* `0`: Iris setosa\n",
    "* `1`: Iris versicolor\n",
    "* `2`: Iris virginica\n",
    "\n",
    "For more information about features and labels, see the [ML Terminology section of the Machine Learning Crash Course](https://developers.google.com/machine-learning/crash-course/framing/ml-terminology)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sVNlJlUOhkoX"
   },
   "outputs": [],
   "source": [
    "class_names = ['Iris setosa', 'Iris versicolor', 'Iris virginica']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dqPkQExM2Pwt"
   },
   "source": [
    "### Create a `tf.data.Dataset`\n",
    "\n",
    "TensorFlow's [Dataset API](../../guide/data.ipynb) handles many common cases for loading data into a model. This is a high-level API for reading data and transforming it into a form used for training.\n",
    "\n",
    "\n",
    "Since the dataset is a CSV-formatted text file, use the `tf.data.experimental.make_csv_dataset` function to parse the data into a suitable format. Since this function generates data for training models, the default behavior is to shuffle the data (`shuffle=True, shuffle_buffer_size=10000`), and repeat the dataset forever (`num_epochs=None`). We also set the [batch_size](https://developers.google.com/machine-learning/glossary/#batch_size) parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WsxHnz1ebJ2S"
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_dataset = tf.data.experimental.make_csv_dataset(\n",
    "    train_dataset_fp,\n",
    "    batch_size,\n",
    "    column_names=column_names,\n",
    "    label_name=label_name,\n",
    "    num_epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gB_RSn62c-3G"
   },
   "source": [
    "The `make_csv_dataset` function returns a `tf.data.Dataset` of `(features, label)` pairs, where `features` is a dictionary: `{'feature_name': value}`\n",
    "\n",
    "These `Dataset` objects are iterable. Let's look at a batch of features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iDuG94H-C122"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('sepal_length', <tf.Tensor: shape=(32,), dtype=float32, numpy=\n",
      "array([6.6, 7.2, 5.1, 5. , 5.1, 5. , 6. , 5.4, 6.3, 6.9, 5.4, 7.2, 5.7,\n",
      "       7.4, 6.5, 5. , 5.7, 6.2, 6.3, 7.3, 7.9, 6.1, 7.7, 4.9, 6.5, 6.6,\n",
      "       6.7, 4.6, 6.5, 5.9, 6.2, 5.8], dtype=float32)>), ('sepal_width', <tf.Tensor: shape=(32,), dtype=float32, numpy=\n",
      "array([2.9, 3.2, 2.5, 3.5, 3.5, 3. , 3. , 3.9, 2.5, 3.1, 3. , 3. , 2.8,\n",
      "       2.8, 3. , 2. , 2.8, 2.8, 3.3, 2.9, 3.8, 2.6, 2.8, 3.1, 3. , 3. ,\n",
      "       3.1, 3.6, 2.8, 3.2, 2.2, 4. ], dtype=float32)>), ('petal_length', <tf.Tensor: shape=(32,), dtype=float32, numpy=\n",
      "array([4.6, 6. , 3. , 1.6, 1.4, 1.6, 4.8, 1.3, 5. , 4.9, 4.5, 5.8, 4.1,\n",
      "       6.1, 5.2, 3.5, 4.5, 4.8, 4.7, 6.3, 6.4, 5.6, 6.7, 1.5, 5.5, 4.4,\n",
      "       5.6, 1. , 4.6, 4.8, 4.5, 1.2], dtype=float32)>), ('petal_width', <tf.Tensor: shape=(32,), dtype=float32, numpy=\n",
      "array([1.3, 1.8, 1.1, 0.6, 0.3, 0.2, 1.8, 0.4, 1.9, 1.5, 1.5, 1.6, 1.3,\n",
      "       1.9, 2. , 1. , 1.3, 1.8, 1.6, 1.8, 2. , 1.4, 2. , 0.1, 1.8, 1.4,\n",
      "       2.4, 0.2, 1.5, 1.8, 1.5, 0.2], dtype=float32)>)]) tf.Tensor([1 2 1 0 0 0 2 0 2 1 1 2 1 2 2 1 1 2 1 2 2 2 2 0 2 1 2 0 1 1 1 0], shape=(32,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "features, labels = next(iter(train_dataset))\n",
    "\n",
    "print(features,labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E63mArnQaAGz"
   },
   "source": [
    "Notice that like-features are grouped together, or *batched*. Each example row's fields are appended to the corresponding feature array. Change the `batch_size` to set the number of examples stored in these feature arrays.\n",
    "\n",
    "You can start to see some clusters by plotting a few features from the batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "me5Wn-9FcyyO"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(features['petal_length'],\n",
    "            features['sepal_length'],\n",
    "            c=labels,\n",
    "            cmap='viridis')\n",
    "\n",
    "plt.xlabel(\"Petal length\")\n",
    "plt.ylabel(\"Sepal length\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YlxpSyHlhT6M"
   },
   "source": [
    "To simplify the model building step, create a function to repackage the features dictionary into a single array with shape: `(batch_size, num_features)`.\n",
    "\n",
    "This function uses the `tf.stack` method which takes values from a list of tensors and creates a combined tensor at the specified dimension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jm932WINcaGU"
   },
   "outputs": [],
   "source": [
    "def pack_features_vector(features, labels):\n",
    "  \"\"\"Pack the features into a single array.\"\"\"\n",
    "  features = tf.stack(list(features.values()), axis=1)\n",
    "  return features, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V1Vuph_eDl8x"
   },
   "source": [
    "Then use the `tf.data.Dataset#map` method to pack the `features` of each `(features,label)` pair into the training dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZbDkzGZIkpXf"
   },
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.map(pack_features_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NLy0Q1xCldVO"
   },
   "source": [
    "The features element of the `Dataset` are now arrays with shape `(batch_size, num_features)`. Let's look at the first few examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kex9ibEek6Tr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 4)\n",
      "tf.Tensor(\n",
      "[[4.4 3.2 1.3 0.2]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [6.  2.9 4.5 1.5]\n",
      " [7.2 3.6 6.1 2.5]\n",
      " [4.9 3.1 1.5 0.1]], shape=(5, 4), dtype=float32) tf.Tensor([0 0 1 2 0], shape=(5,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "features, labels = next(iter(train_dataset))\n",
    "print(features.shape)\n",
    "print(features[:5],labels[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LsaVrtNM3Tx5"
   },
   "source": [
    "## Select the type of model\n",
    "\n",
    "### Why model?\n",
    "\n",
    "A *[model](https://developers.google.com/machine-learning/crash-course/glossary#model)* is a relationship between features and the label.  For the Iris classification problem, the model defines the relationship between the sepal and petal measurements and the predicted Iris species. Some simple models can be described with a few lines of algebra, but complex machine learning models have a large number of parameters that are difficult to summarize.\n",
    "\n",
    "Could you determine the relationship between the four features and the Iris species *without* using machine learning?  That is, could you use traditional programming techniques (for example, a lot of conditional statements) to create a model?  Perhapsâ€”if you analyzed the dataset long enough to determine the relationships between petal and sepal measurements to a particular species. And this becomes difficultâ€”maybe impossibleâ€”on more complicated datasets. A good machine learning approach *determines the model for you*. If you feed enough representative examples into the right machine learning model type, the program will figure out the relationships for you.\n",
    "\n",
    "### Select the model\n",
    "\n",
    "We need to select the kind of model to train. There are many types of models and picking a good one takes experience. This tutorial uses a neural network to solve the Iris classification problem. *[Neural networks](https://developers.google.com/machine-learning/glossary/#neural_network)* can find complex relationships between features and the label. It is a highly-structured graph, organized into one or more *[hidden layers](https://developers.google.com/machine-learning/glossary/#hidden_layer)*. Each hidden layer consists of one or more *[neurons](https://developers.google.com/machine-learning/glossary/#neuron)*. There are several categories of neural networks and this program uses a dense, or *[fully-connected neural network](https://developers.google.com/machine-learning/glossary/#fully_connected_layer)*: the neurons in one layer receive input connections from *every* neuron in the previous layer. For example, Figure 2 illustrates a dense neural network consisting of an input layer, two hidden layers, and an output layer:\n",
    "\n",
    "<table>\n",
    "  <tr><td>\n",
    "    <img src=\"https://www.tensorflow.org/images/custom_estimators/full_network.png\"\n",
    "         alt=\"A diagram of the network architecture: Inputs, 2 hidden layers, and outputs\">\n",
    "  </td></tr>\n",
    "  <tr><td align=\"center\">\n",
    "    <b>Figure 2.</b> A neural network with features, hidden layers, and predictions.<br/>&nbsp;\n",
    "  </td></tr>\n",
    "</table>\n",
    "\n",
    "When the model from Figure 2 is trained and fed an unlabeled example, it yields three predictions: the likelihood that this flower is the given Iris species. This prediction is called *[inference](https://developers.google.com/machine-learning/crash-course/glossary#inference)*. For this example, the sum of the output predictions is 1.0. In Figure 2, this prediction breaks down as: `0.02` for *Iris setosa*, `0.95` for *Iris versicolor*, and `0.03` for *Iris virginica*. This means that the model predictsâ€”with 95% probabilityâ€”that an unlabeled example flower is an *Iris versicolor*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W23DIMVPQEBt"
   },
   "source": [
    "### Create a model using Keras\n",
    "\n",
    "The TensorFlow `tf.keras` API is the preferred way to create models and layers. This makes it easy to build models and experiment while Keras handles the complexity of connecting everything together.\n",
    "\n",
    "The `tf.keras.Sequential` model is a linear stack of layers. Its constructor takes a list of layer instances, in this case, two `tf.keras.layers.Dense` layers with 10 nodes each, and an output layer with 3 nodes representing our label predictions. The first layer's `input_shape` parameter corresponds to the number of features from the dataset, and is required:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2fZ6oL2ig3ZK"
   },
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "  tf.keras.layers.Dense(10, activation=tf.nn.relu, input_shape=(4,)),  # input shape required\n",
    "  tf.keras.layers.Dense(10, activation=tf.nn.relu),\n",
    "  tf.keras.layers.Dense(3)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='sgd',loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FHcbEzMpxbHL"
   },
   "source": [
    "The *[activation function](https://developers.google.com/machine-learning/crash-course/glossary#activation_function)* determines the output shape of each node in the layer. These non-linearities are importantâ€”without them the model would be equivalent to a single layer. There are many `tf.keras.activations`, but [ReLU](https://developers.google.com/machine-learning/crash-course/glossary#ReLU) is common for hidden layers.\n",
    "\n",
    "The ideal number of hidden layers and neurons depends on the problem and the dataset. Like many aspects of machine learning, picking the best shape of the neural network requires a mixture of knowledge and experimentation. As a rule of thumb, increasing the number of hidden layers and neurons typically creates a more powerful model, which requires more data to train effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1725\n",
      "Epoch 2/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1723\n",
      "Epoch 3/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1720\n",
      "Epoch 4/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1718\n",
      "Epoch 5/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1715\n",
      "Epoch 6/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1712\n",
      "Epoch 7/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1710\n",
      "Epoch 8/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1707\n",
      "Epoch 9/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1704\n",
      "Epoch 10/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1702\n",
      "Epoch 11/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1699\n",
      "Epoch 12/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1697\n",
      "Epoch 13/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1694\n",
      "Epoch 14/200\n",
      "1/1 [==============================] - 0s 959us/step - loss: 0.1691\n",
      "Epoch 15/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1689\n",
      "Epoch 16/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1686\n",
      "Epoch 17/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1684\n",
      "Epoch 18/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1681\n",
      "Epoch 19/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1679\n",
      "Epoch 20/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1676\n",
      "Epoch 21/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1674\n",
      "Epoch 22/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1671\n",
      "Epoch 23/200\n",
      "1/1 [==============================] - 0s 995us/step - loss: 0.1669\n",
      "Epoch 24/200\n",
      "1/1 [==============================] - 0s 978us/step - loss: 0.1666\n",
      "Epoch 25/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1664\n",
      "Epoch 26/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1661\n",
      "Epoch 27/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1659\n",
      "Epoch 28/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1656\n",
      "Epoch 29/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1654\n",
      "Epoch 30/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1651\n",
      "Epoch 31/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1649\n",
      "Epoch 32/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1647\n",
      "Epoch 33/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1644\n",
      "Epoch 34/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1642\n",
      "Epoch 35/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1639\n",
      "Epoch 36/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1637\n",
      "Epoch 37/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1635\n",
      "Epoch 38/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1632\n",
      "Epoch 39/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1630\n",
      "Epoch 40/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1627\n",
      "Epoch 41/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1625\n",
      "Epoch 42/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1623\n",
      "Epoch 43/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1620\n",
      "Epoch 44/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1618\n",
      "Epoch 45/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1615\n",
      "Epoch 46/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1613\n",
      "Epoch 47/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1611\n",
      "Epoch 48/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1608\n",
      "Epoch 49/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1606\n",
      "Epoch 50/200\n",
      "1/1 [==============================] - 0s 897us/step - loss: 0.1604\n",
      "Epoch 51/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1601\n",
      "Epoch 52/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1599\n",
      "Epoch 53/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1597\n",
      "Epoch 54/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1594\n",
      "Epoch 55/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1592\n",
      "Epoch 56/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1590\n",
      "Epoch 57/200\n",
      "1/1 [==============================] - 0s 987us/step - loss: 0.1588\n",
      "Epoch 58/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1585\n",
      "Epoch 59/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1583\n",
      "Epoch 60/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1581\n",
      "Epoch 61/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1578\n",
      "Epoch 62/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1576\n",
      "Epoch 63/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1574\n",
      "Epoch 64/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1572\n",
      "Epoch 65/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1569\n",
      "Epoch 66/200\n",
      "1/1 [==============================] - 0s 927us/step - loss: 0.1567\n",
      "Epoch 67/200\n",
      "1/1 [==============================] - 0s 919us/step - loss: 0.1565\n",
      "Epoch 68/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1562\n",
      "Epoch 69/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1560\n",
      "Epoch 70/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1558\n",
      "Epoch 71/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1556\n",
      "Epoch 72/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1554\n",
      "Epoch 73/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1551\n",
      "Epoch 74/200\n",
      "1/1 [==============================] - 0s 876us/step - loss: 0.1549\n",
      "Epoch 75/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1547\n",
      "Epoch 76/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1545\n",
      "Epoch 77/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1542\n",
      "Epoch 78/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1540\n",
      "Epoch 79/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1538\n",
      "Epoch 80/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1536\n",
      "Epoch 81/200\n",
      "1/1 [==============================] - 0s 954us/step - loss: 0.1534\n",
      "Epoch 82/200\n",
      "1/1 [==============================] - 0s 976us/step - loss: 0.1531\n",
      "Epoch 83/200\n",
      "1/1 [==============================] - 0s 760us/step - loss: 0.1529\n",
      "Epoch 84/200\n",
      "1/1 [==============================] - 0s 869us/step - loss: 0.1527\n",
      "Epoch 85/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1525\n",
      "Epoch 86/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1523\n",
      "Epoch 87/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1521\n",
      "Epoch 88/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1518\n",
      "Epoch 89/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1516\n",
      "Epoch 90/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1514\n",
      "Epoch 91/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1512\n",
      "Epoch 92/200\n",
      "1/1 [==============================] - 0s 995us/step - loss: 0.1510\n",
      "Epoch 93/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1508\n",
      "Epoch 94/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1505\n",
      "Epoch 95/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1503\n",
      "Epoch 96/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1501\n",
      "Epoch 97/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1499\n",
      "Epoch 98/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1497\n",
      "Epoch 99/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1495\n",
      "Epoch 100/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1493\n",
      "Epoch 101/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1491\n",
      "Epoch 102/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1488\n",
      "Epoch 103/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1486\n",
      "Epoch 104/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1484\n",
      "Epoch 105/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1482\n",
      "Epoch 106/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1480\n",
      "Epoch 107/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1478\n",
      "Epoch 108/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1476\n",
      "Epoch 109/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1474\n",
      "Epoch 110/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1472\n",
      "Epoch 111/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1470\n",
      "Epoch 112/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1468\n",
      "Epoch 113/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1466\n",
      "Epoch 114/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1463\n",
      "Epoch 115/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1461\n",
      "Epoch 116/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1459\n",
      "Epoch 117/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1457\n",
      "Epoch 118/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1455\n",
      "Epoch 119/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1453\n",
      "Epoch 120/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1451\n",
      "Epoch 121/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1449\n",
      "Epoch 122/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1447\n",
      "Epoch 123/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1445\n",
      "Epoch 124/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1443\n",
      "Epoch 125/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1441\n",
      "Epoch 126/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1439\n",
      "Epoch 127/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1437\n",
      "Epoch 128/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1435\n",
      "Epoch 129/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1433\n",
      "Epoch 130/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1431\n",
      "Epoch 131/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1429\n",
      "Epoch 132/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1427\n",
      "Epoch 133/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1425\n",
      "Epoch 134/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1423\n",
      "Epoch 135/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1421\n",
      "Epoch 136/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1419\n",
      "Epoch 137/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1417\n",
      "Epoch 138/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1415\n",
      "Epoch 139/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1413\n",
      "Epoch 140/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1411\n",
      "Epoch 141/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1409\n",
      "Epoch 142/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1407\n",
      "Epoch 143/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1405\n",
      "Epoch 144/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1403\n",
      "Epoch 145/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1401\n",
      "Epoch 146/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1400\n",
      "Epoch 147/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1398\n",
      "Epoch 148/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1396\n",
      "Epoch 149/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1394\n",
      "Epoch 150/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1392\n",
      "Epoch 151/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1390\n",
      "Epoch 152/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1388\n",
      "Epoch 153/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1386\n",
      "Epoch 154/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1384\n",
      "Epoch 155/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1382\n",
      "Epoch 156/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1380\n",
      "Epoch 157/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1378\n",
      "Epoch 158/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1377\n",
      "Epoch 159/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1375\n",
      "Epoch 160/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1373\n",
      "Epoch 161/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1371\n",
      "Epoch 162/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1369\n",
      "Epoch 163/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1367\n",
      "Epoch 164/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1365\n",
      "Epoch 165/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1363\n",
      "Epoch 166/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1361\n",
      "Epoch 167/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1360\n",
      "Epoch 168/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1358\n",
      "Epoch 169/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1356\n",
      "Epoch 170/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1354\n",
      "Epoch 171/200\n",
      "1/1 [==============================] - 0s 939us/step - loss: 0.1352\n",
      "Epoch 172/200\n",
      "1/1 [==============================] - 0s 862us/step - loss: 0.1350\n",
      "Epoch 173/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1349\n",
      "Epoch 174/200\n",
      "1/1 [==============================] - 0s 968us/step - loss: 0.1347\n",
      "Epoch 175/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1345\n",
      "Epoch 176/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1343\n",
      "Epoch 177/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1341\n",
      "Epoch 178/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1339\n",
      "Epoch 179/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1338\n",
      "Epoch 180/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1336\n",
      "Epoch 181/200\n",
      "1/1 [==============================] - 0s 875us/step - loss: 0.1334\n",
      "Epoch 182/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1332\n",
      "Epoch 183/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1330\n",
      "Epoch 184/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1328\n",
      "Epoch 185/200\n",
      "1/1 [==============================] - 0s 977us/step - loss: 0.1327\n",
      "Epoch 186/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1325\n",
      "Epoch 187/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1323\n",
      "Epoch 188/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1321\n",
      "Epoch 189/200\n",
      "1/1 [==============================] - 0s 971us/step - loss: 0.1319\n",
      "Epoch 190/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1318\n",
      "Epoch 191/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1316\n",
      "Epoch 192/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1314\n",
      "Epoch 193/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1312\n",
      "Epoch 194/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1311\n",
      "Epoch 195/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1309\n",
      "Epoch 196/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1307\n",
      "Epoch 197/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1305\n",
      "Epoch 198/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1304\n",
      "Epoch 199/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1302\n",
      "Epoch 200/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1300\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1af0a2610>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(features, labels,epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2wFKnhWCpDSS"
   },
   "source": [
    "### Using the model\n",
    "\n",
    "Let's have a quick look at what this model does to a batch of features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xe6SQ5NrpB-I"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 3), dtype=float32, numpy=\n",
       "array([[-0.298091  ,  0.6433698 ,  0.26667255],\n",
       "       [-0.42753297,  0.9096902 ,  0.604336  ],\n",
       "       [-0.37848926,  0.6645691 ,  0.3328426 ],\n",
       "       [-0.29396537,  0.96137524, -0.33831796],\n",
       "       [-0.3057607 ,  0.97784495, -0.32970947]], dtype=float32)>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model(features)\n",
    "predictions[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wxyXOhwVr5S3"
   },
   "source": [
    "Here, each example returns a [logit](https://developers.google.com/machine-learning/crash-course/glossary#logits) for each class.\n",
    "\n",
    "To convert these logits to a probability for each class, use the [softmax](https://developers.google.com/machine-learning/crash-course/glossary#softmax) function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_tRwHZmTNTX2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 3), dtype=float32, numpy=\n",
       "array([[0.18787262, 0.48165348, 0.3304739 ],\n",
       "       [0.13132392, 0.500141  , 0.3685351 ],\n",
       "       [0.17022479, 0.48307806, 0.34669718],\n",
       "       [0.18296087, 0.64201576, 0.17502338],\n",
       "       [0.17902002, 0.6461963 , 0.17478365]], dtype=float32)>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.nn.softmax(predictions[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uRZmchElo481"
   },
   "source": [
    "Taking the `tf.argmax` across classes gives us the predicted class index. But, the model hasn't been trained yet, so these aren't good predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-Jzm_GoErz8B"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "    Labels: [1 2 1 0 0 1 0 2 0 2 0 2 1 0 1 2 2 2 0 2 2 1 0 2 2 0 1 0 2 2 1 0]\n"
     ]
    }
   ],
   "source": [
    "print(\"Prediction: {}\".format(tf.argmax(predictions, axis=1)))\n",
    "print(\"    Labels: {}\".format(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vzq2E5J2QMtw"
   },
   "source": [
    "## Train the model\n",
    "\n",
    "*[Training](https://developers.google.com/machine-learning/crash-course/glossary#training)* is the stage of machine learning when the model is gradually optimized, or the model *learns* the dataset. The goal is to learn enough about the structure of the training dataset to make predictions about unseen data. If you learn *too much* about the training dataset, then the predictions only work for the data it has seen and will not be generalizable. This problem is called *[overfitting](https://developers.google.com/machine-learning/crash-course/glossary#overfitting)*â€”it's like memorizing the answers instead of understanding how to solve a problem.\n",
    "\n",
    "The Iris classification problem is an example of *[supervised machine learning](https://developers.google.com/machine-learning/glossary/#supervised_machine_learning)*: the model is trained from examples that contain labels. In *[unsupervised machine learning](https://developers.google.com/machine-learning/glossary/#unsupervised_machine_learning)*, the examples don't contain labels. Instead, the model typically finds patterns among the features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RaKp8aEjKX6B"
   },
   "source": [
    "### Define the loss and gradient function\n",
    "\n",
    "Both training and evaluation stages need to calculate the model's *[loss](https://developers.google.com/machine-learning/crash-course/glossary#loss)*. This measures how off a model's predictions are from the desired label, in other words, how bad the model is performing. We want to minimize, or optimize, this value.\n",
    "\n",
    "Our model will calculate its loss using the `tf.keras.losses.SparseCategoricalCrossentropy` function which takes the model's class probability predictions and the desired label, and returns the average loss across the examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QOsi6b-1CXIn"
   },
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tMAT4DcMPwI-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss test: 1.1634917259216309\n"
     ]
    }
   ],
   "source": [
    "def loss(model, x, y, training):\n",
    "  # training=training is needed only if there are layers with different\n",
    "  # behavior during training versus inference (e.g. Dropout).\n",
    "  y_ = model(x, training=training)\n",
    "\n",
    "  return loss_object(y_true=y, y_pred=y_)\n",
    "\n",
    "\n",
    "l = loss(model, features, labels, training=False)\n",
    "print(\"Loss test: {}\".format(l))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3IcPqA24QM6B"
   },
   "source": [
    "Use the `tf.GradientTape` context to calculate the *[gradients](https://developers.google.com/machine-learning/crash-course/glossary#gradient)* used to optimize your model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x57HcKWhKkei"
   },
   "outputs": [],
   "source": [
    "def grad(model, inputs, targets):\n",
    "  with tf.GradientTape() as tape:\n",
    "    loss_value = loss(model, inputs, targets, training=True)\n",
    "  return loss_value, tape.gradient(loss_value, model.trainable_variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lOxFimtlKruu"
   },
   "source": [
    "### Create an optimizer\n",
    "\n",
    "An *[optimizer](https://developers.google.com/machine-learning/crash-course/glossary#optimizer)* applies the computed gradients to the model's variables to minimize the `loss` function. You can think of the loss function as a curved surface (see Figure 3) and we want to find its lowest point by walking around. The gradients point in the direction of steepest ascentâ€”so we'll travel the opposite way and move down the hill. By iteratively calculating the loss and gradient for each batch, we'll adjust the model during training. Gradually, the model will find the best combination of weights and bias to minimize loss. And the lower the loss, the better the model's predictions.\n",
    "\n",
    "<table>\n",
    "  <tr><td>\n",
    "    <img src=\"https://cs231n.github.io/assets/nn3/opt1.gif\" width=\"70%\"\n",
    "         alt=\"Optimization algorithms visualized over time in 3D space.\">\n",
    "  </td></tr>\n",
    "  <tr><td align=\"center\">\n",
    "    <b>Figure 3.</b> Optimization algorithms visualized over time in 3D space.<br/>(Source: <a href=\"http://cs231n.github.io/neural-networks-3/\">Stanford class CS231n</a>, MIT License, Image credit: <a href=\"https://twitter.com/alecrad\">Alec Radford</a>)\n",
    "  </td></tr>\n",
    "</table>\n",
    "\n",
    "TensorFlow has many optimization algorithms available for training. This model uses the `tf.keras.optimizers.SGD` that implements the *[stochastic gradient descent](https://developers.google.com/machine-learning/crash-course/glossary#gradient_descent)* (SGD) algorithm. The `learning_rate` sets the step size to take for each iteration down the hill. This is a *hyperparameter* that you'll commonly adjust to achieve better results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XkUd6UiZa_dF"
   },
   "source": [
    "Let's setup the optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8xxi2NNGKwG_"
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pJVRZ0hP52ZB"
   },
   "source": [
    "We'll use this to calculate a single optimization step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rxRNTFVe56RG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0, Initial Loss: 1.1634917259216309\n",
      "Step: 1,         Loss: 1.1392085552215576\n"
     ]
    }
   ],
   "source": [
    "loss_value, grads = grad(model, features, labels)\n",
    "\n",
    "print(\"Step: {}, Initial Loss: {}\".format(optimizer.iterations.numpy(),\n",
    "                                          loss_value.numpy()))\n",
    "\n",
    "optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "print(\"Step: {},         Loss: {}\".format(optimizer.iterations.numpy(),\n",
    "                                          loss(model, features, labels, training=True).numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7Y2VSELvwAvW"
   },
   "source": [
    "### Training loop\n",
    "\n",
    "With all the pieces in place, the model is ready for training! A training loop feeds the dataset examples into the model to help it make better predictions. The following code block sets up these training steps:\n",
    "\n",
    "1. Iterate each *epoch*. An epoch is one pass through the dataset.\n",
    "2. Within an epoch, iterate over each example in the training `Dataset` grabbing its *features* (`x`) and *label* (`y`).\n",
    "3. Using the example's features, make a prediction and compare it with the label. Measure the inaccuracy of the prediction and use that to calculate the model's loss and gradients.\n",
    "4. Use an `optimizer` to update the model's variables.\n",
    "5. Keep track of some stats for visualization.\n",
    "6. Repeat for each epoch.\n",
    "\n",
    "The `num_epochs` variable is the number of times to loop over the dataset collection. Counter-intuitively, training a model longer does not guarantee a better model. `num_epochs` is a *[hyperparameter](https://developers.google.com/machine-learning/glossary/#hyperparameter)* that you can tune. Choosing the right number usually requires both experience and experimentation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AIgulGRUhpto"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 000: Loss: 1.111, Accuracy: 30.000%\n",
      "Epoch 050: Loss: 0.593, Accuracy: 70.000%\n",
      "Epoch 100: Loss: 0.400, Accuracy: 88.333%\n",
      "Epoch 150: Loss: 0.301, Accuracy: 95.000%\n",
      "Epoch 200: Loss: 0.220, Accuracy: 97.500%\n"
     ]
    }
   ],
   "source": [
    "## Note: Rerunning this cell uses the same model variables\n",
    "\n",
    "# Keep results for plotting\n",
    "train_loss_results = []\n",
    "train_accuracy_results = []\n",
    "\n",
    "num_epochs = 201\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "  epoch_loss_avg = tf.keras.metrics.Mean()\n",
    "  epoch_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "  # Training loop - using batches of 32\n",
    "  for x, y in train_dataset:\n",
    "    # Optimize the model\n",
    "    loss_value, grads = grad(model, x, y)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "    # Track progress\n",
    "    epoch_loss_avg.update_state(loss_value)  # Add current batch loss\n",
    "    # Compare predicted label to actual label\n",
    "    # training=True is needed only if there are layers with different\n",
    "    # behavior during training versus inference (e.g. Dropout).\n",
    "    epoch_accuracy.update_state(y, model(x, training=True))\n",
    "\n",
    "  # End epoch\n",
    "  train_loss_results.append(epoch_loss_avg.result())\n",
    "  train_accuracy_results.append(epoch_accuracy.result())\n",
    "\n",
    "  if epoch % 50 == 0:\n",
    "    print(\"Epoch {:03d}: Loss: {:.3f}, Accuracy: {:.3%}\".format(epoch,\n",
    "                                                                epoch_loss_avg.result(),\n",
    "                                                                epoch_accuracy.result()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2FQHVUnm_rjw"
   },
   "source": [
    "### Visualize the loss function over time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j3wdbmtLVTyr"
   },
   "source": [
    "While it's helpful to print out the model's training progress, it's often *more* helpful to see this progress. [TensorBoard](https://www.tensorflow.org/tensorboard) is a nice visualization tool that is packaged with TensorFlow, but we can create basic charts using the `matplotlib` module.\n",
    "\n",
    "Interpreting these charts takes some experience, but you really want to see the *loss* go down and the *accuracy* go up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "agjvNd2iUGFn"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(2, sharex=True, figsize=(12, 8))\n",
    "fig.suptitle('Training Metrics')\n",
    "\n",
    "axes[0].set_ylabel(\"Loss\", fontsize=14)\n",
    "axes[0].plot(train_loss_results)\n",
    "\n",
    "axes[1].set_ylabel(\"Accuracy\", fontsize=14)\n",
    "axes[1].set_xlabel(\"Epoch\", fontsize=14)\n",
    "axes[1].plot(train_accuracy_results)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zg8GoMZhLpGH"
   },
   "source": [
    "## Evaluate the model's effectiveness\n",
    "\n",
    "Now that the model is trained, we can get some statistics on its performance.\n",
    "\n",
    "*Evaluating* means determining how effectively the model makes predictions. To determine the model's effectiveness at Iris classification, pass some sepal and petal measurements to the model and ask the model to predict what Iris species they represent. Then compare the model's predictions against the actual label.  For example, a model that picked the correct species on half the input examples has an *[accuracy](https://developers.google.com/machine-learning/glossary/#accuracy)* of `0.5`. Figure 4 shows a slightly more effective model, getting 4 out of 5 predictions correct at 80% accuracy:\n",
    "\n",
    "<table cellpadding=\"8\" border=\"0\">\n",
    "  <colgroup>\n",
    "    <col span=\"4\" >\n",
    "    <col span=\"1\" bgcolor=\"lightblue\">\n",
    "    <col span=\"1\" bgcolor=\"lightgreen\">\n",
    "  </colgroup>\n",
    "  <tr bgcolor=\"lightgray\">\n",
    "    <th colspan=\"4\">Example features</th>\n",
    "    <th colspan=\"1\">Label</th>\n",
    "    <th colspan=\"1\" >Model prediction</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>5.9</td><td>3.0</td><td>4.3</td><td>1.5</td><td align=\"center\">1</td><td align=\"center\">1</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>6.9</td><td>3.1</td><td>5.4</td><td>2.1</td><td align=\"center\">2</td><td align=\"center\">2</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>5.1</td><td>3.3</td><td>1.7</td><td>0.5</td><td align=\"center\">0</td><td align=\"center\">0</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>6.0</td> <td>3.4</td> <td>4.5</td> <td>1.6</td> <td align=\"center\">1</td><td align=\"center\" bgcolor=\"red\">2</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>5.5</td><td>2.5</td><td>4.0</td><td>1.3</td><td align=\"center\">1</td><td align=\"center\">1</td>\n",
    "  </tr>\n",
    "  <tr><td align=\"center\" colspan=\"6\">\n",
    "    <b>Figure 4.</b> An Iris classifier that is 80% accurate.<br/>&nbsp;\n",
    "  </td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z-EvK7hGL0d8"
   },
   "source": [
    "### Setup the test dataset\n",
    "\n",
    "Evaluating the model is similar to training the model. The biggest difference is the examples come from a separate *[test set](https://developers.google.com/machine-learning/crash-course/glossary#test_set)* rather than the training set. To fairly assess a model's effectiveness, the examples used to evaluate a model must be different from the examples used to train the model.\n",
    "\n",
    "The setup for the test `Dataset` is similar to the setup for training `Dataset`. Download the CSV text file and parse that values, then give it a little shuffle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ps3_9dJ3Lodk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/iris_test.csv\n",
      "8192/573 [============================================================================================================================================================================================================================================================================================================================================================================================================================================] - 0s 1us/step\n"
     ]
    }
   ],
   "source": [
    "test_url = \"https://storage.googleapis.com/download.tensorflow.org/data/iris_test.csv\"\n",
    "\n",
    "test_fp = tf.keras.utils.get_file(fname=os.path.basename(test_url),\n",
    "                                  origin=test_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SRMWCu30bnxH"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(30, 4), dtype=float32, numpy=\n",
       " array([[5.9, 3. , 4.2, 1.5],\n",
       "        [6.9, 3.1, 5.4, 2.1],\n",
       "        [5.1, 3.3, 1.7, 0.5],\n",
       "        [6. , 3.4, 4.5, 1.6],\n",
       "        [5.5, 2.5, 4. , 1.3],\n",
       "        [6.2, 2.9, 4.3, 1.3],\n",
       "        [5.5, 4.2, 1.4, 0.2],\n",
       "        [6.3, 2.8, 5.1, 1.5],\n",
       "        [5.6, 3. , 4.1, 1.3],\n",
       "        [6.7, 2.5, 5.8, 1.8],\n",
       "        [7.1, 3. , 5.9, 2.1],\n",
       "        [4.3, 3. , 1.1, 0.1],\n",
       "        [5.6, 2.8, 4.9, 2. ],\n",
       "        [5.5, 2.3, 4. , 1.3],\n",
       "        [6. , 2.2, 4. , 1. ],\n",
       "        [5.1, 3.5, 1.4, 0.2],\n",
       "        [5.7, 2.6, 3.5, 1. ],\n",
       "        [4.8, 3.4, 1.9, 0.2],\n",
       "        [5.1, 3.4, 1.5, 0.2],\n",
       "        [5.7, 2.5, 5. , 2. ],\n",
       "        [5.4, 3.4, 1.7, 0.2],\n",
       "        [5.6, 3. , 4.5, 1.5],\n",
       "        [6.3, 2.9, 5.6, 1.8],\n",
       "        [6.3, 2.5, 4.9, 1.5],\n",
       "        [5.8, 2.7, 3.9, 1.2],\n",
       "        [6.1, 3. , 4.6, 1.4],\n",
       "        [5.2, 4.1, 1.5, 0.1],\n",
       "        [6.7, 3.1, 4.7, 1.5],\n",
       "        [6.7, 3.3, 5.7, 2.5],\n",
       "        [6.4, 2.9, 4.3, 1.3]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(30,), dtype=int32, numpy=\n",
       " array([1, 2, 0, 1, 1, 1, 0, 2, 1, 2, 2, 0, 2, 1, 1, 0, 1, 0, 0, 2, 0, 1,\n",
       "        2, 1, 1, 1, 0, 1, 2, 1], dtype=int32)>)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset = tf.data.experimental.make_csv_dataset(\n",
    "    test_fp,\n",
    "    batch_size,\n",
    "    column_names=column_names,\n",
    "    label_name='species',\n",
    "    num_epochs=1,\n",
    "    shuffle=False)\n",
    "\n",
    "test_dataset = test_dataset.map(pack_features_vector)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HFuOKXJdMAdm"
   },
   "source": [
    "### Evaluate the model on the test dataset\n",
    "\n",
    "Unlike the training stage, the model only evaluates a single [epoch](https://developers.google.com/machine-learning/glossary/#epoch) of the test data. In the following code cell, we iterate over each example in the test set and compare the model's prediction against the actual label. This is used to measure the model's accuracy across the entire test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#simple\n",
    "test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Tw03-MK1cYId"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy: 96.667%\n"
     ]
    }
   ],
   "source": [
    "test_accuracy = tf.keras.metrics.Accuracy()\n",
    "\n",
    "for (x, y) in test_dataset:\n",
    "  # training=False is needed only if there are layers with different\n",
    "  # behavior during training versus inference (e.g. Dropout).\n",
    "  logits = model(x, training=False)\n",
    "  prediction = tf.argmax(logits, axis=1, output_type=tf.int32)\n",
    "  test_accuracy(prediction, y)\n",
    "\n",
    "print(\"Test set accuracy: {:.3%}\".format(test_accuracy.result()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HcKEZMtCOeK-"
   },
   "source": [
    "We can see on the last batch, for example, the model is usually correct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uNwt2eMeOane"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(30, 2), dtype=int32, numpy=\n",
       "array([[1, 1],\n",
       "       [2, 2],\n",
       "       [0, 0],\n",
       "       [1, 1],\n",
       "       [1, 1],\n",
       "       [1, 1],\n",
       "       [0, 0],\n",
       "       [2, 2],\n",
       "       [1, 1],\n",
       "       [2, 2],\n",
       "       [2, 2],\n",
       "       [0, 0],\n",
       "       [2, 2],\n",
       "       [1, 1],\n",
       "       [1, 1],\n",
       "       [0, 0],\n",
       "       [1, 1],\n",
       "       [0, 0],\n",
       "       [0, 0],\n",
       "       [2, 2],\n",
       "       [0, 0],\n",
       "       [1, 1],\n",
       "       [2, 2],\n",
       "       [1, 2],\n",
       "       [1, 1],\n",
       "       [1, 1],\n",
       "       [0, 0],\n",
       "       [1, 1],\n",
       "       [2, 2],\n",
       "       [1, 1]], dtype=int32)>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.stack([y,prediction],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7Li2r1tYvW7S"
   },
   "source": [
    "## Use the trained model to make predictions\n",
    "\n",
    "We've trained a model and \"proven\" that it's goodâ€”but not perfectâ€”at classifying Iris species. Now let's use the trained model to make some predictions on [unlabeled examples](https://developers.google.com/machine-learning/glossary/#unlabeled_example); that is, on examples that contain features but not a label.\n",
    "\n",
    "In real-life, the unlabeled examples could come from lots of different sources including apps, CSV files, and data feeds. For now, we're going to manually provide three unlabeled examples to predict their labels. Recall, the label numbers are mapped to a named representation as:\n",
    "\n",
    "* `0`: Iris setosa\n",
    "* `1`: Iris versicolor\n",
    "* `2`: Iris virginica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kesTS5Lzv-M2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 0 prediction: Iris setosa (97.4%)\n",
      "Example 1 prediction: Iris versicolor (74.3%)\n",
      "Example 2 prediction: Iris virginica (74.6%)\n"
     ]
    }
   ],
   "source": [
    "predict_dataset = tf.convert_to_tensor([\n",
    "    [5.1, 3.3, 1.7, 0.5,],\n",
    "    [5.9, 3.0, 4.2, 1.5,],\n",
    "    [6.9, 3.1, 5.4, 2.1]\n",
    "])\n",
    "\n",
    "# training=False is needed only if there are layers with different\n",
    "# behavior during training versus inference (e.g. Dropout).\n",
    "predictions = model(predict_dataset, training=False)\n",
    "\n",
    "for i, logits in enumerate(predictions):\n",
    "  class_idx = tf.argmax(logits).numpy()\n",
    "  p = tf.nn.softmax(logits)[class_idx]\n",
    "  name = class_names[class_idx]\n",
    "  print(\"Example {} prediction: {} ({:4.1f}%)\".format(i, name, 100*p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "03_int_logistic_regression (2).ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit ('rjstf': conda)",
   "language": "python",
   "name": "python394jvsc74a57bd098a5c86d771fd654659942e3026542ca0c5d1b9177e5786b6dfe6af420dae8b9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
